{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "80517dbc",
      "cell_type": "markdown",
      "source": [
        "# GRPO Training project: teach an LLM to do additions, again"
      ],
      "metadata": {
        "id": "80517dbc"
      }
    },
    {
      "id": "DAEbh3jBadc7",
      "cell_type": "markdown",
      "source": [
        "In this notebook, you'll find:\n",
        "* A basic Transformer with basic tokenizer\n",
        "* A basic dataset for additions\n",
        "* A classical pre-trainer, minimizing cross entropy loss\n",
        "* A Vanilla GRPO"
      ],
      "metadata": {
        "id": "DAEbh3jBadc7"
      }
    },
    {
      "id": "MB8lj855LgHl",
      "cell_type": "markdown",
      "source": [
        "You're not supposed to edit the existing code (you can if you want to...).\n",
        "You should implement one (or more) of the following:\n",
        "* GRPO with PPO (the `usual` one)\n",
        "* RLOO\n",
        "* ReMax\n",
        "* DPO\n",
        "* RAFT\n",
        "* your own RLHF method!"
      ],
      "metadata": {
        "id": "MB8lj855LgHl"
      }
    },
    {
      "id": "298013ba-48dc-480e-88c0-cb81bd87f236",
      "cell_type": "markdown",
      "source": [
        "### Task and Commment on the work\n",
        "\n",
        "we ve implemented GRPO with PPO using the slides of the class.\n",
        "You can see how in the `compute_loss_clip_KL`function"
      ],
      "metadata": {
        "id": "298013ba-48dc-480e-88c0-cb81bd87f236"
      }
    },
    {
      "id": "ae993bb9",
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time"
      ],
      "metadata": {
        "id": "ae993bb9",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:21.861035Z",
          "iopub.execute_input": "2025-03-11T10:35:21.861559Z",
          "iopub.status.idle": "2025-03-11T10:35:27.825539Z",
          "shell.execute_reply.started": "2025-03-11T10:35:21.861528Z",
          "shell.execute_reply": "2025-03-11T10:35:27.824657Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "OzGh9ahKF17h",
      "cell_type": "code",
      "source": [
        "num_digits = 3\n",
        "\n",
        "dataset_size = 64_000\n",
        "train_proportion = 0.9"
      ],
      "metadata": {
        "id": "OzGh9ahKF17h",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:27.826593Z",
          "iopub.execute_input": "2025-03-11T10:35:27.827016Z",
          "iopub.status.idle": "2025-03-11T10:35:27.830593Z",
          "shell.execute_reply.started": "2025-03-11T10:35:27.826982Z",
          "shell.execute_reply": "2025-03-11T10:35:27.829820Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "fabd151a",
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fabd151a",
        "outputId": "e7b36965-8f38-4c49-cf03-0109ab723eca",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:27.832098Z",
          "iopub.execute_input": "2025-03-11T10:35:27.832426Z",
          "iopub.status.idle": "2025-03-11T10:35:27.926968Z",
          "shell.execute_reply.started": "2025-03-11T10:35:27.832400Z",
          "shell.execute_reply": "2025-03-11T10:35:27.925936Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "cuda\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "6c054bed",
      "cell_type": "markdown",
      "source": [
        "## Step 1: Construct a tokenizer"
      ],
      "metadata": {
        "id": "6c054bed"
      }
    },
    {
      "id": "t6aC9uNeIR6C",
      "cell_type": "code",
      "source": [
        "pad_token=\"[PAD]\"\n",
        "eos_token=\"[EOS]\""
      ],
      "metadata": {
        "id": "t6aC9uNeIR6C",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:27.928178Z",
          "iopub.execute_input": "2025-03-11T10:35:27.928494Z",
          "iopub.status.idle": "2025-03-11T10:35:27.947768Z",
          "shell.execute_reply.started": "2025-03-11T10:35:27.928462Z",
          "shell.execute_reply": "2025-03-11T10:35:27.946947Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "g2QiF-otFur3",
      "cell_type": "code",
      "source": [
        "class character_level_tokenizer:\n",
        "    \"\"\"\n",
        "    character-level\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.vocab = [str(x) for x in range(10)] + [\"+\", \"=\"] + [pad_token, eos_token]\n",
        "        self.token_to_id = {v : k for k, v in enumerate(self.vocab)}\n",
        "        self.id_to_token = {k : v for k, v in enumerate(self.vocab)}\n",
        "        self.ntokens = len(self.vocab)\n",
        "        self.pattern = f\"[^{re.escape(''.join(self.vocab))}]\"\n",
        "\n",
        "    def clean(self, text):\n",
        "        \"\"\"\n",
        "        removes all characters not in the vocabulary\n",
        "        \"\"\"\n",
        "        out = re.sub(self.pattern, \"\", text)\n",
        "        return out\n",
        "\n",
        "    def pre_tokenization(self, text):\n",
        "        \"\"\"\n",
        "        character-level\n",
        "        \"\"\"\n",
        "        return [c for c in text]\n",
        "\n",
        "    def encode(self, text):\n",
        "        text_list = self.pre_tokenization(self.clean(text))\n",
        "        return [self.token_to_id[c] for c in text_list]\n",
        "\n",
        "    def decode(self, token_list):\n",
        "        return \"\".join([self.id_to_token[x] for x in token_list])"
      ],
      "metadata": {
        "id": "g2QiF-otFur3",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:27.948643Z",
          "iopub.execute_input": "2025-03-11T10:35:27.948889Z",
          "iopub.status.idle": "2025-03-11T10:35:27.969016Z",
          "shell.execute_reply.started": "2025-03-11T10:35:27.948871Z",
          "shell.execute_reply": "2025-03-11T10:35:27.968443Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "QuCc6jF5F8hK",
      "cell_type": "code",
      "source": [
        "tokenizer = character_level_tokenizer()\n",
        "ntokens = tokenizer.ntokens\n",
        "ntokens"
      ],
      "metadata": {
        "id": "QuCc6jF5F8hK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e42dddaa-dacd-471e-b7d0-48f17004dd87",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:27.969783Z",
          "iopub.execute_input": "2025-03-11T10:35:27.970050Z",
          "iopub.status.idle": "2025-03-11T10:35:27.989419Z",
          "shell.execute_reply.started": "2025-03-11T10:35:27.970023Z",
          "shell.execute_reply": "2025-03-11T10:35:27.988768Z"
        }
      },
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "14"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "8FXW2K-1Jd-P",
      "cell_type": "code",
      "source": [
        "prompt = \"12 + 42 =\"\n",
        "inputs = tokenizer.encode(prompt)\n",
        "inputs, tokenizer.decode(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FXW2K-1Jd-P",
        "outputId": "fff6c129-8820-4b3c-8caa-b43710370098",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:27.990039Z",
          "iopub.execute_input": "2025-03-11T10:35:27.990256Z",
          "iopub.status.idle": "2025-03-11T10:35:28.009908Z",
          "shell.execute_reply.started": "2025-03-11T10:35:27.990239Z",
          "shell.execute_reply": "2025-03-11T10:35:28.009126Z"
        }
      },
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "([1, 2, 10, 4, 2, 11], '12+42=')"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "491af297",
      "cell_type": "markdown",
      "source": [
        "## Step 2: Create a dataset for arithmetic operations"
      ],
      "metadata": {
        "id": "491af297"
      }
    },
    {
      "id": "daa90f31",
      "cell_type": "code",
      "source": [
        "def sample_datapoint(num_digits = 3):\n",
        "    a_list = [random.randint(0, 9) for _ in range(num_digits)]\n",
        "    b_list = [random.randint(0, 9) for _ in range(num_digits)]\n",
        "    a_int = int(\"\".join([str(x) for x in a_list]))\n",
        "    b_int = int(\"\".join([str(x) for x in b_list]))\n",
        "    a_str = \"\".join([str(x) for x in a_list])\n",
        "    b_str = \"\".join([str(x) for x in b_list])\n",
        "    sum_int = a_int + b_int\n",
        "    return (a_str + \"+\" + b_str + \"=\", str(sum_int))\n",
        "\n",
        "sample_datapoint(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daa90f31",
        "outputId": "c23f0b74-6948-43a9-9e6f-5846f4b325b4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:28.012296Z",
          "iopub.execute_input": "2025-03-11T10:35:28.012482Z",
          "iopub.status.idle": "2025-03-11T10:35:28.031944Z",
          "shell.execute_reply.started": "2025-03-11T10:35:28.012466Z",
          "shell.execute_reply": "2025-03-11T10:35:28.031201Z"
        }
      },
      "outputs": [
        {
          "execution_count": 8,
          "output_type": "execute_result",
          "data": {
            "text/plain": "('224+830=', '1054')"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "b6e861d2",
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for _ in range(dataset_size):\n",
        "    data.append(sample_datapoint(num_digits))\n",
        "data[:4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6e861d2",
        "outputId": "7b639165-7809-4f2d-a067-f21547c293b1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:28.033426Z",
          "iopub.execute_input": "2025-03-11T10:35:28.033654Z",
          "iopub.status.idle": "2025-03-11T10:35:28.686038Z",
          "shell.execute_reply.started": "2025-03-11T10:35:28.033635Z",
          "shell.execute_reply": "2025-03-11T10:35:28.685271Z"
        }
      },
      "outputs": [
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[('248+095=', '343'),\n ('827+001=', '828'),\n ('946+690=', '1636'),\n ('229+865=', '1094')]"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "fee85050",
      "cell_type": "code",
      "source": [
        "data_train = data[: int(train_proportion * dataset_size)]\n",
        "data_test = data[int(train_proportion * dataset_size):]\n",
        "\n",
        "len(data_train),len(data_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fee85050",
        "outputId": "69def39c-084c-4074-f296-908925038748",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:28.686876Z",
          "iopub.execute_input": "2025-03-11T10:35:28.687188Z",
          "iopub.status.idle": "2025-03-11T10:35:28.693877Z",
          "shell.execute_reply.started": "2025-03-11T10:35:28.687140Z",
          "shell.execute_reply": "2025-03-11T10:35:28.693093Z"
        }
      },
      "outputs": [
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(57600, 6400)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "37200598",
      "cell_type": "markdown",
      "source": [
        "## Step 3: Construct a model"
      ],
      "metadata": {
        "id": "37200598"
      }
    },
    {
      "id": "91674239",
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
        "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
        "        Here, we use sine and cosine functions of different frequencies.\n",
        "    .. math:\n",
        "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
        "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
        "        \\text{where pos is the word position and i is the embed idx)\n",
        "    Args:\n",
        "        d_model: the embed dim (required).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        max_len: the max. length of the incoming sequence (default=5000).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        r\"\"\"Inputs of forward function\n",
        "        Args:\n",
        "            x: the sequence fed to the positional encoder model (required).\n",
        "        Shape:\n",
        "            x: [sequence length, batch size, embed dim]\n",
        "            output: [sequence length, batch size, embed dim]\n",
        "        \"\"\"\n",
        "\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "91674239",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:28.694547Z",
          "iopub.execute_input": "2025-03-11T10:35:28.694735Z",
          "iopub.status.idle": "2025-03-11T10:35:28.714030Z",
          "shell.execute_reply.started": "2025-03-11T10:35:28.694719Z",
          "shell.execute_reply": "2025-03-11T10:35:28.713239Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4eb278ab",
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Transformer):\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__(d_model=ninp,\n",
        "                                               nhead=nhead,\n",
        "                                               dim_feedforward=nhid,\n",
        "                                               num_encoder_layers=nlayers)\n",
        "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.ninp = ninp\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        return torch.log(torch.tril(torch.ones(sz,sz)))\n",
        "\n",
        "    def forward(self, src):\n",
        "        mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "        self.src_mask = mask\n",
        "\n",
        "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output_enc = self.encoder(src, mask=self.src_mask)\n",
        "        output_dec = self.decoder(output_enc)\n",
        "        return F.log_softmax(output_dec, dim=-1), output_enc"
      ],
      "metadata": {
        "id": "4eb278ab",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:28.714738Z",
          "iopub.execute_input": "2025-03-11T10:35:28.714914Z",
          "iopub.status.idle": "2025-03-11T10:35:28.733310Z",
          "shell.execute_reply.started": "2025-03-11T10:35:28.714899Z",
          "shell.execute_reply": "2025-03-11T10:35:28.732517Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1d568cc4",
      "cell_type": "code",
      "source": [
        "model = TransformerModel(ntoken = ntokens,\n",
        "                         ninp = 128,\n",
        "                         nhead = 16,\n",
        "                         nhid = 64,\n",
        "                         nlayers = 8)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d568cc4",
        "outputId": "7677a7ea-d06a-46a8-85f3-44274cc008a5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:28.734057Z",
          "iopub.execute_input": "2025-03-11T10:35:28.734330Z",
          "iopub.status.idle": "2025-03-11T10:35:29.203644Z",
          "shell.execute_reply.started": "2025-03-11T10:35:28.734313Z",
          "shell.execute_reply": "2025-03-11T10:35:29.202713Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TransformerModel(\n  (encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0-7): 8 x TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n        )\n        (linear1): Linear(in_features=128, out_features=64, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=64, out_features=128, bias=True)\n        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n  )\n  (decoder): Linear(in_features=128, out_features=14, bias=True)\n  (input_emb): Embedding(14, 128)\n  (pos_encoder): PositionalEncoding(\n    (dropout): Dropout(p=0.5, inplace=False)\n  )\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "a6PmJSo95N4C",
      "cell_type": "code",
      "source": [
        "print(\"number of parameters: {}\".format(sum([x.numel() for x in model.parameters()])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6PmJSo95N4C",
        "outputId": "f0177342-43b6-46c6-b079-aab40db326d1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:29.204506Z",
          "iopub.execute_input": "2025-03-11T10:35:29.204782Z",
          "iopub.status.idle": "2025-03-11T10:35:29.209507Z",
          "shell.execute_reply.started": "2025-03-11T10:35:29.204754Z",
          "shell.execute_reply": "2025-03-11T10:35:29.208669Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "number of parameters: 668942\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "2e35d113",
      "cell_type": "markdown",
      "source": [
        "### Useful functions"
      ],
      "metadata": {
        "id": "2e35d113"
      }
    },
    {
      "id": "8f2f06e0",
      "cell_type": "code",
      "source": [
        "def generate(model, prompts, new_tokens = 5, mode = \"greedy\", num_samples = 1, temperature = 0.8):\n",
        "    input_tensor = torch.repeat_interleave(prompts, repeats = num_samples, dim = 1).to(device)\n",
        "    # (prompt_length, batch_size * num_samples)\n",
        "    for _ in range(new_tokens):\n",
        "        output, _ = model(input_tensor) # (prompt_length, batch_size * num_samples, ntokens)\n",
        "        logits = output[-1,:,:] # (batch_size * num_samples, ntokens)\n",
        "        if mode == \"greedy\":\n",
        "            tokens = torch.argmax(logits, -1).view((1,-1)) # (1, batch_size * num_samples)\n",
        "        else: # mode == \"sampling\"\n",
        "            logits /= temperature\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            tokens = torch.multinomial(probs, num_samples = 1).view((1,-1)) # (1, batch_size * num_samples)\n",
        "        input_tensor = torch.cat((input_tensor, tokens), 0)\n",
        "    return input_tensor"
      ],
      "metadata": {
        "id": "8f2f06e0",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:29.210243Z",
          "iopub.execute_input": "2025-03-11T10:35:29.210479Z",
          "iopub.status.idle": "2025-03-11T10:35:29.226749Z",
          "shell.execute_reply.started": "2025-03-11T10:35:29.210462Z",
          "shell.execute_reply": "2025-03-11T10:35:29.226194Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d76d1b19",
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "prompt = \"2+3=\"\n",
        "prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
        "output = generate(model, prompt_tensor).view((1,-1))\n",
        "output, tokenizer.decode(output[0].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d76d1b19",
        "outputId": "7c084e50-8269-4f24-859a-4f95f8bd143d",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:29.227358Z",
          "iopub.execute_input": "2025-03-11T10:35:29.227539Z",
          "iopub.status.idle": "2025-03-11T10:35:30.077210Z",
          "shell.execute_reply.started": "2025-03-11T10:35:29.227524Z",
          "shell.execute_reply": "2025-03-11T10:35:30.076466Z"
        }
      },
      "outputs": [
        {
          "execution_count": 16,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(tensor([[ 2, 10,  3, 11, 12, 12, 12, 12, 12]], device='cuda:0'),\n '2+3=[PAD][PAD][PAD][PAD][PAD]')"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "00954ddc",
      "cell_type": "code",
      "source": [
        "def pad(token_list, type_list = \"prompts\"):\n",
        "    max_length = max([len(x) for x in token_list])\n",
        "    out = []\n",
        "    for x in token_list:\n",
        "        if type_list == \"prompts\":\n",
        "            out.append([tokenizer.token_to_id[pad_token]] * (max_length - len(x)) + x)\n",
        "        if type_list == \"answers\":\n",
        "            out.append(x + [tokenizer.token_to_id[eos_token]] + [tokenizer.token_to_id[pad_token]] * (max_length - len(x)))\n",
        "    return out, max_length"
      ],
      "metadata": {
        "id": "00954ddc",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:30.078131Z",
          "iopub.execute_input": "2025-03-11T10:35:30.078522Z",
          "iopub.status.idle": "2025-03-11T10:35:30.083006Z",
          "shell.execute_reply.started": "2025-03-11T10:35:30.078490Z",
          "shell.execute_reply": "2025-03-11T10:35:30.082357Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2c84beab",
      "cell_type": "code",
      "source": [
        "prompts = [tokenizer.encode(\"1+1=\"), tokenizer.encode(\"21+35=\")]\n",
        "answers = [tokenizer.encode(\"2\"), tokenizer.encode(\"56\")]\n",
        "padded_prompts, _ = pad(prompts, \"prompts\")\n",
        "padded_answers, _ = pad(answers, \"answers\")\n",
        "padded_prompts, padded_answers\n",
        "[tokenizer.decode(p) for p in padded_prompts], [tokenizer.decode(p) for p in padded_answers]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c84beab",
        "outputId": "3e53a680-31e1-4cd6-8ce0-d96009def972",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:30.083887Z",
          "iopub.execute_input": "2025-03-11T10:35:30.084132Z",
          "iopub.status.idle": "2025-03-11T10:35:30.103040Z",
          "shell.execute_reply.started": "2025-03-11T10:35:30.084114Z",
          "shell.execute_reply": "2025-03-11T10:35:30.102439Z"
        }
      },
      "outputs": [
        {
          "execution_count": 18,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(['[PAD][PAD]1+1=', '21+35='], ['2[EOS][PAD]', '56[EOS]'])"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "264f9227",
      "cell_type": "code",
      "source": [
        "def get_batch(split, i, batch_size):\n",
        "    data = data_train if split == 'train' else data_test\n",
        "\n",
        "    prompts = [data[i][0] for i in range(i, i + batch_size)]\n",
        "    encoded_prompts = [tokenizer.encode(prompt) for prompt in prompts]\n",
        "    padded_prompts, prompt_length = pad(encoded_prompts, \"prompts\")\n",
        "\n",
        "    answers = [data[i][1] for i in range(i, i + batch_size)]\n",
        "    encoded_answers = [tokenizer.encode(answer) for answer in answers]\n",
        "    padded_answers, answers_length = pad(encoded_answers, \"answers\")\n",
        "\n",
        "    X = torch.stack([torch.tensor(x) for x in padded_prompts], 1)\n",
        "    Y = torch.stack([torch.tensor(x) for x in padded_answers], 1)\n",
        "    return X, Y, prompt_length, answers_length, prompts, answers"
      ],
      "metadata": {
        "id": "264f9227",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:30.103784Z",
          "iopub.execute_input": "2025-03-11T10:35:30.104017Z",
          "iopub.status.idle": "2025-03-11T10:35:30.123465Z",
          "shell.execute_reply.started": "2025-03-11T10:35:30.103993Z",
          "shell.execute_reply": "2025-03-11T10:35:30.122692Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "91e281ad",
      "cell_type": "code",
      "source": [
        "X, Y, prompt_length, answers_length, prompts, answers = get_batch(\"train\", 43, 16)\n",
        "X.shape, Y.shape, prompt_length, answers_length, prompts[0], answers[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91e281ad",
        "outputId": "2a97779d-08e9-485a-9588-07e699dc6b77",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:30.124227Z",
          "iopub.execute_input": "2025-03-11T10:35:30.124408Z",
          "iopub.status.idle": "2025-03-11T10:35:30.149259Z",
          "shell.execute_reply.started": "2025-03-11T10:35:30.124393Z",
          "shell.execute_reply": "2025-03-11T10:35:30.148646Z"
        }
      },
      "outputs": [
        {
          "execution_count": 20,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(torch.Size([8, 16]), torch.Size([5, 16]), 8, 4, '798+180=', '978')"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "113e1fd1",
      "cell_type": "markdown",
      "source": [
        "## Step 4: Evaluate"
      ],
      "metadata": {
        "id": "113e1fd1"
      }
    },
    {
      "id": "KfmcSdPwp3K6",
      "cell_type": "code",
      "source": [
        "batch_size = 16"
      ],
      "metadata": {
        "id": "KfmcSdPwp3K6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:30.149975Z",
          "iopub.execute_input": "2025-03-11T10:35:30.150221Z",
          "iopub.status.idle": "2025-03-11T10:35:30.166798Z",
          "shell.execute_reply.started": "2025-03-11T10:35:30.150197Z",
          "shell.execute_reply": "2025-03-11T10:35:30.166228Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1cfcd10a",
      "cell_type": "code",
      "source": [
        "def evaluate(batch_size = batch_size):\n",
        "    # Turn on evaluation mode disables dropout.\n",
        "    model.eval()\n",
        "    correct = 0.\n",
        "    with torch.no_grad():\n",
        "        for batch, i in enumerate(range(0, len(data_test) - 1, batch_size)):\n",
        "            prompts, target_answers, prompt_length, answers_length, _, _ = get_batch(\"test\", i, batch_size)\n",
        "            prompts = prompts.to(device) # (prompt_length, batch_size)\n",
        "            target_answers = target_answers.to(device) # (answers_length + 1, batch_size)\n",
        "            output = generate(model, prompts, answers_length + 1) # (prompt_length + answers_length + 1, batch_size)\n",
        "            answers_tokens = output[prompt_length:, :] # (answers_length + 1, batch_size), contains tokens\n",
        "            equality_test = answers_tokens == target_answers # (answers_length + 1, batch_size), contains boolean values\n",
        "            correct += torch.all(equality_test, axis=0).float().sum()\n",
        "        accuracy = correct / len(data_test)\n",
        "    return accuracy.item()"
      ],
      "metadata": {
        "id": "1cfcd10a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:30.167456Z",
          "iopub.execute_input": "2025-03-11T10:35:30.167672Z",
          "iopub.status.idle": "2025-03-11T10:35:30.186753Z",
          "shell.execute_reply.started": "2025-03-11T10:35:30.167655Z",
          "shell.execute_reply": "2025-03-11T10:35:30.186189Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ac335b05",
      "cell_type": "code",
      "source": [
        "evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac335b05",
        "outputId": "1355d497-45f0-440a-90b8-30a7f2818091",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:30.190217Z",
          "iopub.execute_input": "2025-03-11T10:35:30.190412Z",
          "iopub.status.idle": "2025-03-11T10:35:39.907890Z",
          "shell.execute_reply.started": "2025-03-11T10:35:30.190396Z",
          "shell.execute_reply": "2025-03-11T10:35:39.907218Z"
        }
      },
      "outputs": [
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.0"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "4c54061a",
      "cell_type": "markdown",
      "source": [
        "## Step 5: Train the model, classical approach"
      ],
      "metadata": {
        "id": "4c54061a"
      }
    },
    {
      "id": "b827e567",
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ],
      "metadata": {
        "id": "b827e567"
      }
    },
    {
      "id": "5b140ba3",
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "batch_size = 16\n",
        "learning_rate = 8e-4\n",
        "\n",
        "reporting_per_epoch = 5\n",
        "log_interval = len(data_train) // (reporting_per_epoch + 1)\n",
        "assert(log_interval % batch_size == 0)"
      ],
      "metadata": {
        "id": "5b140ba3",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:39.909133Z",
          "iopub.execute_input": "2025-03-11T10:35:39.909369Z",
          "iopub.status.idle": "2025-03-11T10:35:39.913039Z",
          "shell.execute_reply.started": "2025-03-11T10:35:39.909350Z",
          "shell.execute_reply": "2025-03-11T10:35:39.912323Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3638a75d",
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    best_test_accuracy = None\n",
        "    test_accuracy = evaluate()\n",
        "    print('-' * 89)\n",
        "    print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
        "    print('-' * 89)\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        total_loss = 0.\n",
        "        start_time = time.time()\n",
        "        for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
        "            prompts, target_answers, prompt_length, answers_length, _, _ = get_batch(\"train\", i, batch_size)\n",
        "            prompts = prompts.to(device) # (prompt_length, batch_size)\n",
        "            target_answers = target_answers.to(device) # (answers_length + 1, batch_size)\n",
        "            input_tensor = torch.cat((prompts, target_answers), 0) # (prompt_length + answers_length + 1, batch_size)\n",
        "            model.zero_grad()\n",
        "            output, _ = model(input_tensor) # (prompt_length + answers_length + 1, batch_size, ntokens)\n",
        "            output_answers = output[prompt_length-1:-1,:,:].reshape(-1, ntokens) # ((answers_length + 1) * batch_size, ntokens)\n",
        "            target_answers = target_answers.view(-1)\n",
        "            loss = F.cross_entropy(output_answers, target_answers)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if i % log_interval == 0 and batch > 0:\n",
        "                cur_loss = total_loss / log_interval\n",
        "                elapsed = time.time() - start_time\n",
        "                print('| {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.2f} | perplexity {:8.2f}'.format(batch, len(data_train) // batch_size,\n",
        "                                                                                                            elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "                total_loss = 0\n",
        "                start_time = time.time()\n",
        "        test_accuracy = evaluate()\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the test accuracy is the best we've seen so far.\n",
        "        if not best_test_accuracy or test_accuracy < best_test_accuracy:\n",
        "            with open(\"arithmetic.pt\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_test_accuracy = test_accuracy"
      ],
      "metadata": {
        "id": "3638a75d",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:39.913905Z",
          "iopub.execute_input": "2025-03-11T10:35:39.914206Z",
          "iopub.status.idle": "2025-03-11T10:35:39.940976Z",
          "shell.execute_reply.started": "2025-03-11T10:35:39.914178Z",
          "shell.execute_reply": "2025-03-11T10:35:39.940440Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4e2a8490",
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e2a8490",
        "outputId": "492562cc-d243-4314-ab56-d17d41c070ad",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:35:39.941741Z",
          "iopub.execute_input": "2025-03-11T10:35:39.941957Z",
          "iopub.status.idle": "2025-03-11T10:41:40.900058Z",
          "shell.execute_reply.started": "2025-03-11T10:35:39.941940Z",
          "shell.execute_reply": "2025-03-11T10:41:40.899101Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "-----------------------------------------------------------------------------------------\n| initialisation | test accuracy  0.00\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch  1.08 | loss  0.09 | perplexity     1.09\n|  1200/ 3600 batches | ms/batch  1.08 | loss  0.07 | perplexity     1.07\n|  1800/ 3600 batches | ms/batch  1.03 | loss  0.07 | perplexity     1.07\n|  2400/ 3600 batches | ms/batch  1.03 | loss  0.07 | perplexity     1.07\n|  3000/ 3600 batches | ms/batch  1.04 | loss  0.07 | perplexity     1.07\n-----------------------------------------------------------------------------------------\n| end of epoch   1 | time: 69.64s | test accuracy  0.01\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch  1.09 | loss  0.07 | perplexity     1.07\n|  1200/ 3600 batches | ms/batch  1.05 | loss  0.07 | perplexity     1.07\n|  1800/ 3600 batches | ms/batch  1.04 | loss  0.07 | perplexity     1.07\n|  2400/ 3600 batches | ms/batch  1.06 | loss  0.06 | perplexity     1.07\n|  3000/ 3600 batches | ms/batch  1.05 | loss  0.06 | perplexity     1.06\n-----------------------------------------------------------------------------------------\n| end of epoch   2 | time: 70.61s | test accuracy  0.01\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch  1.04 | loss  0.06 | perplexity     1.06\n|  1200/ 3600 batches | ms/batch  1.05 | loss  0.06 | perplexity     1.06\n|  1800/ 3600 batches | ms/batch  1.06 | loss  0.05 | perplexity     1.06\n|  2400/ 3600 batches | ms/batch  1.05 | loss  0.05 | perplexity     1.05\n|  3000/ 3600 batches | ms/batch  1.02 | loss  0.04 | perplexity     1.05\n-----------------------------------------------------------------------------------------\n| end of epoch   3 | time: 69.54s | test accuracy  0.08\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch  1.07 | loss  0.05 | perplexity     1.05\n|  1200/ 3600 batches | ms/batch  1.04 | loss  0.04 | perplexity     1.04\n|  1800/ 3600 batches | ms/batch  1.02 | loss  0.04 | perplexity     1.04\n|  2400/ 3600 batches | ms/batch  1.04 | loss  0.04 | perplexity     1.04\n|  3000/ 3600 batches | ms/batch  1.01 | loss  0.04 | perplexity     1.04\n-----------------------------------------------------------------------------------------\n| end of epoch   4 | time: 68.96s | test accuracy  0.10\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch  1.02 | loss  0.03 | perplexity     1.03\n|  1200/ 3600 batches | ms/batch  1.05 | loss  0.04 | perplexity     1.04\n|  1800/ 3600 batches | ms/batch  1.02 | loss  0.03 | perplexity     1.03\n|  2400/ 3600 batches | ms/batch  1.03 | loss  0.03 | perplexity     1.03\n|  3000/ 3600 batches | ms/batch  1.04 | loss  0.04 | perplexity     1.04\n-----------------------------------------------------------------------------------------\n| end of epoch   5 | time: 68.48s | test accuracy  0.19\n-----------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "56d9d440",
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "for i in range(20):\n",
        "    prompt, answers = data_test[i]\n",
        "    prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
        "    output = generate(model, prompt_tensor, len(answers) + 1).view((1,-1))\n",
        "    print(tokenizer.decode(output.tolist()[0]) + \"\\t actual result: \" + answers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56d9d440",
        "outputId": "b876390d-4396-4a95-e600-e2230317dbec",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:41:40.901244Z",
          "iopub.execute_input": "2025-03-11T10:41:40.901681Z",
          "iopub.status.idle": "2025-03-11T10:41:41.381359Z",
          "shell.execute_reply.started": "2025-03-11T10:41:40.901657Z",
          "shell.execute_reply": "2025-03-11T10:41:41.380614Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "473+062=536[EOS]\t actual result: 535\n952+626=1578[EOS]\t actual result: 1578\n047+298=342[EOS]\t actual result: 345\n684+038=722[EOS]\t actual result: 722\n998+913=1912[EOS]\t actual result: 1911\n631+917=1548[EOS]\t actual result: 1548\n633+018=652[EOS]\t actual result: 651\n955+024=978[EOS]\t actual result: 979\n923+715=1638[EOS]\t actual result: 1638\n258+551=806[EOS]\t actual result: 809\n003+139=146[EOS]\t actual result: 142\n647+546=1192[EOS]\t actual result: 1193\n375+991=1369[EOS]\t actual result: 1366\n095+163=258[EOS]\t actual result: 258\n197+060=258[EOS]\t actual result: 257\n461+411=872[EOS]\t actual result: 872\n317+864=1182[EOS]\t actual result: 1181\n294+007=302[EOS]\t actual result: 301\n910+393=1302[EOS]\t actual result: 1303\n161+206=369[EOS]\t actual result: 367\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "cfa4c591",
      "cell_type": "markdown",
      "source": [
        "## Step 4 bis: Vanilla GRPO training\n",
        "\n",
        "## Step 4 bis bis: PPO GRPO training\n",
        "\n",
        "We'll use the PPO model. Actually the part we change is the computation loss"
      ],
      "metadata": {
        "id": "cfa4c591"
      }
    },
    {
      "id": "aff83f72",
      "cell_type": "markdown",
      "source": [
        "### Custom reward functions"
      ],
      "metadata": {
        "id": "aff83f72"
      }
    },
    {
      "id": "3c548bf7",
      "cell_type": "code",
      "source": [
        "def accuracy_reward(output, answer):\n",
        "    pattern = r\"\\[EOS\\]\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    pattern = r\"(\\[PAD\\])*$\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    return 1. if output == answer else 0.\n",
        "\n",
        "accuracy_reward(\"123[EOS][PAD][PAD]\", \"123\"), accuracy_reward(\"123\", \"124\"),"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c548bf7",
        "outputId": "b5d45345-22eb-445a-b888-6ad22e96d7b7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:41:41.382273Z",
          "iopub.execute_input": "2025-03-11T10:41:41.382602Z",
          "iopub.status.idle": "2025-03-11T10:41:41.389286Z",
          "shell.execute_reply.started": "2025-03-11T10:41:41.382566Z",
          "shell.execute_reply": "2025-03-11T10:41:41.388470Z"
        }
      },
      "outputs": [
        {
          "execution_count": 28,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(1.0, 0.0)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "e1f02762",
      "cell_type": "code",
      "source": [
        "def distance_accuracy_reward(output, answer):\n",
        "    pattern = r\"\\[EOS\\]\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    pattern = r\"(\\[PAD\\])*$\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    int_output = int(output)\n",
        "    int_answer = int(answer)\n",
        "    return abs(int_output - int_answer) / max(int_output, int_answer)\n",
        "\n",
        "distance_accuracy_reward(\"123[EOS]\", \"123\"), distance_accuracy_reward(\"123[PAD]\", \"124\"),"
      ],
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1f02762",
        "outputId": "7271bdf4-4ecf-44ee-ec86-8c815ffaa9a5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:41:41.390120Z",
          "iopub.execute_input": "2025-03-11T10:41:41.390463Z",
          "iopub.status.idle": "2025-03-11T10:41:41.411807Z",
          "shell.execute_reply.started": "2025-03-11T10:41:41.390421Z",
          "shell.execute_reply": "2025-03-11T10:41:41.411048Z"
        }
      },
      "outputs": [
        {
          "execution_count": 29,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(0.0, 0.008064516129032258)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "b42a0d70",
      "cell_type": "code",
      "source": [
        "def digit_accuracy_reward(output, answer):\n",
        "    pattern = r\"\\[EOS\\]\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    pattern = r\"(\\[PAD\\])*$\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    return sum(c1 == c2 for (c1,c2) in zip(output, answer)) / max(len(output), len(answer))\n",
        "\n",
        "digit_accuracy_reward(\"123[EOS][PAD][PAD]\", \"123\"), digit_accuracy_reward(\"123[EOS]\", \"123\"),"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b42a0d70",
        "outputId": "a79ca9d6-a79b-4640-c172-4db78e433256",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:41:41.412567Z",
          "iopub.execute_input": "2025-03-11T10:41:41.412765Z",
          "iopub.status.idle": "2025-03-11T10:41:41.428889Z",
          "shell.execute_reply.started": "2025-03-11T10:41:41.412748Z",
          "shell.execute_reply": "2025-03-11T10:41:41.428228Z"
        }
      },
      "outputs": [
        {
          "execution_count": 30,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(1.0, 1.0)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "a41603b2",
      "cell_type": "code",
      "source": [
        "def reward_format(output):\n",
        "    pattern = r\"\\d+\\[EOS\\](\\[PAD\\])*$\"\n",
        "    return 1. if bool(re.match(pattern, output)) else 0.\n",
        "\n",
        "reward_format(\"123[EOS][PAD][PAD]\"), reward_format(\"123[EOS]\"), reward_format(\"123\"),"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a41603b2",
        "outputId": "33dbf431-4177-4ae3-d36c-08ff8a2261fc",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:41:41.429550Z",
          "iopub.execute_input": "2025-03-11T10:41:41.429723Z",
          "iopub.status.idle": "2025-03-11T10:41:41.448318Z",
          "shell.execute_reply.started": "2025-03-11T10:41:41.429708Z",
          "shell.execute_reply": "2025-03-11T10:41:41.447534Z"
        }
      },
      "outputs": [
        {
          "execution_count": 31,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(1.0, 1.0, 0.0)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "4482e411",
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ],
      "metadata": {
        "id": "4482e411"
      }
    },
    {
      "id": "cf764cb0",
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "batch_size = 16\n",
        "learning_rate = 1e-4\n",
        "num_samples = 16\n",
        "temperature = .8\n",
        "\n",
        "reporting_per_epoch = 5\n",
        "log_interval = len(data_train) // (reporting_per_epoch + 1)\n",
        "assert(log_interval % batch_size == 0)\n",
        "\n",
        "reward_fun = digit_accuracy_reward\n",
        "reward_format = reward_format"
      ],
      "metadata": {
        "id": "cf764cb0",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:41:41.449103Z",
          "iopub.execute_input": "2025-03-11T10:41:41.449557Z",
          "iopub.status.idle": "2025-03-11T10:41:41.479095Z",
          "shell.execute_reply.started": "2025-03-11T10:41:41.449528Z",
          "shell.execute_reply": "2025-03-11T10:41:41.478362Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f15cdced",
      "cell_type": "code",
      "source": [
        "def compute_rewards(text_outputs, answers):\n",
        "    repeated_answers = [answer for answer in answers for _ in range(num_samples)]\n",
        "    rewards = torch.tensor(\n",
        "        [0.2 * reward_format(output) + 0.8 * reward_fun(output, answer)\n",
        "         for output, answer in zip(text_outputs, repeated_answers)],\n",
        "        dtype=torch.float32,\n",
        "        device=device\n",
        "    )\n",
        "    return rewards"
      ],
      "metadata": {
        "id": "f15cdced",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:41:41.479886Z",
          "iopub.execute_input": "2025-03-11T10:41:41.480105Z",
          "iopub.status.idle": "2025-03-11T10:41:41.500997Z",
          "shell.execute_reply.started": "2025-03-11T10:41:41.480081Z",
          "shell.execute_reply": "2025-03-11T10:41:41.500445Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e22be0d4",
      "cell_type": "code",
      "source": [
        "def calculate_grpo_advantages(rewards):\n",
        "    # reshape rewards to group by prompt\n",
        "    # compute mean and standard deviation for each prompt group\n",
        "    mean_rewards = rewards.view(-1, num_samples).mean(dim=1)\n",
        "    std_rewards = rewards.view(-1, num_samples).std(dim=1)\n",
        "    # expand the means and stds to match the original flat rewards tensor shape\n",
        "    mean_rewards = mean_rewards.repeat_interleave(num_samples, dim=0)\n",
        "    std_rewards = std_rewards.repeat_interleave(num_samples, dim=0)\n",
        "    # normalize rewards to get advantages\n",
        "    advantages = (rewards - mean_rewards) / (std_rewards + 1e-5)\n",
        "    return advantages"
      ],
      "metadata": {
        "id": "e22be0d4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:41:41.501884Z",
          "iopub.execute_input": "2025-03-11T10:41:41.502148Z",
          "iopub.status.idle": "2025-03-11T10:41:41.519725Z",
          "shell.execute_reply.started": "2025-03-11T10:41:41.502122Z",
          "shell.execute_reply": "2025-03-11T10:41:41.518946Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "faac5c99",
      "cell_type": "code",
      "source": [
        "def compute_log_probs(model, outputs, prompt_length):\n",
        "    logits, _ = model(outputs)\n",
        "    # logits.shape = (prompt_length + answers_length + 1, batch_size * num_samples, vocab_size)\n",
        "\n",
        "    # we only need the log probabilities for the new tokens\n",
        "    # this introduces a shift: the logits for a position are the predictions for the next token\n",
        "    logits = logits[prompt_length-1:-1, :, :]\n",
        "    # logits.shape = (answers_length + 1, batch_size * num_samples, vocab_size)\n",
        "\n",
        "    # convert raw logits into log probabilities along the vocabulary axis\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    # log_probs.shape = (answers_length + 1, batch_size * num_samples, vocab_size)\n",
        "    return log_probs"
      ],
      "metadata": {
        "id": "faac5c99",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:41:41.520434Z",
          "iopub.execute_input": "2025-03-11T10:41:41.520644Z",
          "iopub.status.idle": "2025-03-11T10:41:41.537520Z",
          "shell.execute_reply.started": "2025-03-11T10:41:41.520617Z",
          "shell.execute_reply": "2025-03-11T10:41:41.536758Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b2612214",
      "cell_type": "code",
      "source": [
        "'''def compute_loss(advantages, log_probs, responses):\n",
        "    # reshape responses from (answers_length + 1, batch_size * num_samples)\n",
        "    # to (answers_length + 1, batch_size * num_samples, 1) for gathering\n",
        "    responses = responses.unsqueeze(-1)\n",
        "    # log_probs.shape = (answers_length + 1, batch_size * num_samples, vocab_size)\n",
        "    # responses.shape = (answers_length + 1, batch_size * num_samples)\n",
        "    # gather the log probability for each token in responses\n",
        "    selected_log_probs = log_probs.gather(dim=-1, index=responses)\n",
        "    # remove the extra last dimension to get back to shape (answers_length + 1, batch_size * num_samples).\n",
        "    selected_log_probs = selected_log_probs.squeeze(-1)\n",
        "\n",
        "    # normalize\n",
        "    selected_log_probs = (selected_log_probs - selected_log_probs.mean(-1, keepdim=True)) / (selected_log_probs.std(-1, keepdim=True) + 1e-5)\n",
        "\n",
        "    # advantages.shape = (batch_size * num_samples)\n",
        "    # we use the same advantages for all tokens in the response\n",
        "    loss = -(advantages.unsqueeze(dim=0) * selected_log_probs).mean()\n",
        "    return loss'''"
      ],
      "metadata": {
        "id": "b2612214",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:41:41.538274Z",
          "iopub.execute_input": "2025-03-11T10:41:41.538536Z",
          "iopub.status.idle": "2025-03-11T10:41:41.558722Z",
          "shell.execute_reply.started": "2025-03-11T10:41:41.538517Z",
          "shell.execute_reply": "2025-03-11T10:41:41.558047Z"
        },
        "outputId": "f4fdb0a3-1ef7-42e9-b878-b9ae52bbacd6"
      },
      "outputs": [
        {
          "execution_count": 36,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'def compute_loss(advantages, log_probs, responses):\\n    # reshape responses from (answers_length + 1, batch_size * num_samples)\\n    # to (answers_length + 1, batch_size * num_samples, 1) for gathering\\n    responses = responses.unsqueeze(-1)\\n    # log_probs.shape = (answers_length + 1, batch_size * num_samples, vocab_size)\\n    # responses.shape = (answers_length + 1, batch_size * num_samples)\\n    # gather the log probability for each token in responses\\n    selected_log_probs = log_probs.gather(dim=-1, index=responses)\\n    # remove the extra last dimension to get back to shape (answers_length + 1, batch_size * num_samples).\\n    selected_log_probs = selected_log_probs.squeeze(-1)\\n\\n    # normalize\\n    selected_log_probs = (selected_log_probs - selected_log_probs.mean(-1, keepdim=True)) / (selected_log_probs.std(-1, keepdim=True) + 1e-5)\\n\\n    # advantages.shape = (batch_size * num_samples)\\n    # we use the same advantages for all tokens in the response\\n    loss = -(advantages.unsqueeze(dim=0) * selected_log_probs).mean()\\n    return loss'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "311c7199-0587-4536-ba50-de3b0b20e77f",
      "cell_type": "code",
      "source": [
        "def compute_loss_clip_KL(advantages, log_probs, old_log_probs, responses, epsilon=0.2, beta=0.03):\n",
        "    responses = responses.unsqueeze(-1)\n",
        "\n",
        "    # Gather log probabilities for selected responses\n",
        "    selected_log_probs = log_probs.gather(dim=-1, index=responses).squeeze(-1)\n",
        "    old_selected_log_probs = old_log_probs.gather(dim=-1, index=responses).squeeze(-1)\n",
        "\n",
        "    # Compute probability ratio\n",
        "    ratio = torch.exp(selected_log_probs - old_selected_log_probs)\n",
        "\n",
        "    # Compute clipped surrogate objective\n",
        "    clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
        "\n",
        "    # Compute PPO loss\n",
        "    advantages_unsqueezed = advantages.unsqueeze(dim=0)  # Unused advantage computation reused\n",
        "    ppo_loss = -torch.min(ratio * advantages_unsqueezed, clipped_ratio * advantages_unsqueezed).mean()\n",
        "\n",
        "    # Compute KL divergence (Regularization Term)\n",
        "    # Using log-probabilities for KL divergence\n",
        "    kl_div = F.kl_div(selected_log_probs.log(), old_selected_log_probs.exp(), reduction='batchmean')\n",
        "\n",
        "    # Total loss: PPO loss + KL penalty\n",
        "    loss = ppo_loss + beta * kl_div\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:41:41.559530Z",
          "iopub.execute_input": "2025-03-11T10:41:41.559787Z",
          "iopub.status.idle": "2025-03-11T10:41:41.580295Z",
          "shell.execute_reply.started": "2025-03-11T10:41:41.559762Z",
          "shell.execute_reply": "2025-03-11T10:41:41.579721Z"
        },
        "id": "311c7199-0587-4536-ba50-de3b0b20e77f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f00ed7dd-82ea-41db-ad1a-f76459cddfdd",
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "f00ed7dd-82ea-41db-ad1a-f76459cddfdd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3c0184c5-64b5-45e4-826a-1b0d983dd403",
      "cell_type": "code",
      "source": [
        "def compute_value(model, prompts):\n",
        "    # Use the model's output to estimate the value function\n",
        "    # You can use the last hidden state or apply a specific layer to output a scalar value\n",
        "    value = model(prompts)  # Assuming the model outputs logits or embeddings\n",
        "    value = value.mean(dim=-1)  # Example: Average over the sequence length to get a scalar value\n",
        "    return value\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:41:41.581107Z",
          "iopub.execute_input": "2025-03-11T10:41:41.581408Z",
          "iopub.status.idle": "2025-03-11T10:41:41.607667Z",
          "shell.execute_reply.started": "2025-03-11T10:41:41.581379Z",
          "shell.execute_reply": "2025-03-11T10:41:41.607093Z"
        },
        "id": "3c0184c5-64b5-45e4-826a-1b0d983dd403"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0a0ba1e3-f127-4a09-9cef-afad044085e8",
      "cell_type": "code",
      "source": [
        "def compute_value(model, prompts):\n",
        "    # Assuming 'model' has a separate value head for the value function\n",
        "    value = model.value_head(prompts)  # Model should output values for each state\n",
        "    return value\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:41:41.608384Z",
          "iopub.execute_input": "2025-03-11T10:41:41.608565Z",
          "iopub.status.idle": "2025-03-11T10:41:41.623270Z",
          "shell.execute_reply.started": "2025-03-11T10:41:41.608551Z",
          "shell.execute_reply": "2025-03-11T10:41:41.622669Z"
        },
        "id": "0a0ba1e3-f127-4a09-9cef-afad044085e8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2183f7ad-62a2-4df3-9802-b161e92cb324",
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "def train_ppo(verbose=False):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    best_test_accuracy = None\n",
        "    test_accuracy = evaluate()\n",
        "    print('-' * 89)\n",
        "    print('| initialization | test accuracy {:5.2f}'.format(test_accuracy))\n",
        "    print('-' * 89)\n",
        "\n",
        "    # switch eval for train model (enables dropout)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        start_time = time.time()\n",
        "        old_log_probs = []  # Store the old log probs\n",
        "        values = []  # Store values from the value network\n",
        "        advantages = []  # Store advantages\n",
        "        rewards = []  # Store rewards\n",
        "\n",
        "        for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
        "\n",
        "            # get a batch of prompts and answers\n",
        "            prompts, _, prompt_length, answers_length, questions, answers = get_batch(\"train\", i, batch_size)\n",
        "            prompts = prompts.to(device)  # (prompt_length, batch_size)\n",
        "\n",
        "            # generate samples for each prompt\n",
        "            outputs = generate(model,\n",
        "                               prompts,\n",
        "                               new_tokens=answers_length + 1,\n",
        "                               mode=\"sampling\",\n",
        "                               num_samples=num_samples,\n",
        "                               temperature=temperature)\n",
        "            # outputs.shape = (prompt_length + answers_length + 1, batch_size * num_samples)\n",
        "            text_outputs = [tokenizer.decode(outputs[prompt_length:, i].tolist())\n",
        "                            for i in range(outputs.size(1))]\n",
        "\n",
        "            # compute rewards\n",
        "            reward = compute_rewards(text_outputs, answers)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            # Compute advantages (here using GAE or some form of advantage calculation)\n",
        "            advantage = calculate_grpo_advantages(reward)\n",
        "            advantages.append(advantage)\n",
        "\n",
        "            # Compute old log probabilities (for PPO ratio)\n",
        "            log_probs = compute_log_probs(model, outputs, prompt_length)\n",
        "            old_log_probs.append(log_probs)\n",
        "\n",
        "            # Compute values from the value network (if applicable)\n",
        "            value = compute_value(model, prompts)  # Assuming model has a value network\n",
        "            values.append(value)\n",
        "\n",
        "\n",
        "        # Convert rewards, advantages, old log_probs to tensors\n",
        "        rewards = torch.tensor(rewards).to(device)\n",
        "        advantages = torch.tensor(advantages).to(device)\n",
        "        old_log_probs = torch.tensor(old_log_probs).to(device)\n",
        "        values = torch.tensor(values).to(device)\n",
        "\n",
        "        # Normalize advantages if necessary\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "        # PPO optimization loop\n",
        "        for _ in range(ppo_epochs):  # PPO typically performs multiple epochs of optimization\n",
        "            for batch in range(len(rewards)):\n",
        "\n",
        "                # Calculate the ratio (new_prob / old_prob)\n",
        "                ratio = torch.exp(log_probs - old_log_probs)\n",
        "\n",
        "                # Calculate surrogate objective with clipping\n",
        "                clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
        "                surrogate_loss = torch.min(ratio * advantages[batch], clipped_ratio * advantages[batch])\n",
        "\n",
        "                # Value loss (typically MSE loss)\n",
        "                value_loss = torch.nn.functional.mse_loss(values[batch], rewards[batch])\n",
        "\n",
        "                # Entropy bonus to encourage exploration (optional)\n",
        "                entropy_loss = -torch.mean(torch.log(ratio))  # Can also be based on entropy from the model's predictions\n",
        "\n",
        "                # Total loss (can include weightings for each term)\n",
        "                loss = -surrogate_loss.meaprinn() + value_loss.mean() - entropy_loss.mean()\n",
        "\n",
        "                # Backpropagation\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        test_accuracy = evaluate()\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
        "        print('-' * 89)\n",
        "\n",
        "        # Save the model if the test accuracy is the best we've seen so far.\n",
        "        if not best_test_accuracy or test_accuracy < best_test_accuracy:\n",
        "            with open(\"arithmetic_ppo.pt\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_test_accuracy = test_accuracy\n",
        "'''"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:41:41.623879Z",
          "iopub.execute_input": "2025-03-11T10:41:41.624050Z",
          "iopub.status.idle": "2025-03-11T10:41:41.643806Z",
          "shell.execute_reply.started": "2025-03-11T10:41:41.624036Z",
          "shell.execute_reply": "2025-03-11T10:41:41.643226Z"
        },
        "id": "2183f7ad-62a2-4df3-9802-b161e92cb324",
        "outputId": "ad7722e5-550a-4920-d998-c25932f06a2c"
      },
      "outputs": [
        {
          "execution_count": 40,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'\\n\\ndef train_ppo(verbose=False):\\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\\n    best_test_accuracy = None\\n    test_accuracy = evaluate()\\n    print(\\'-\\' * 89)\\n    print(\\'| initialization | test accuracy {:5.2f}\\'.format(test_accuracy))\\n    print(\\'-\\' * 89)\\n\\n    # switch eval for train model (enables dropout)\\n    model.train()\\n\\n    for epoch in range(1, epochs + 1):\\n        epoch_start_time = time.time()\\n        start_time = time.time()\\n        old_log_probs = []  # Store the old log probs\\n        values = []  # Store values from the value network\\n        advantages = []  # Store advantages\\n        rewards = []  # Store rewards\\n\\n        for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\\n\\n            # get a batch of prompts and answers\\n            prompts, _, prompt_length, answers_length, questions, answers = get_batch(\"train\", i, batch_size)\\n            prompts = prompts.to(device)  # (prompt_length, batch_size)\\n\\n            # generate samples for each prompt\\n            outputs = generate(model,\\n                               prompts,\\n                               new_tokens=answers_length + 1,\\n                               mode=\"sampling\",\\n                               num_samples=num_samples,\\n                               temperature=temperature)\\n            # outputs.shape = (prompt_length + answers_length + 1, batch_size * num_samples)\\n            text_outputs = [tokenizer.decode(outputs[prompt_length:, i].tolist())\\n                            for i in range(outputs.size(1))]\\n\\n            # compute rewards\\n            reward = compute_rewards(text_outputs, answers)\\n            rewards.append(reward)\\n\\n            # Compute advantages (here using GAE or some form of advantage calculation)\\n            advantage = calculate_grpo_advantages(reward)\\n            advantages.append(advantage)\\n\\n            # Compute old log probabilities (for PPO ratio)\\n            log_probs = compute_log_probs(model, outputs, prompt_length)\\n            old_log_probs.append(log_probs)\\n\\n            # Compute values from the value network (if applicable)\\n            value = compute_value(model, prompts)  # Assuming model has a value network\\n            values.append(value)\\n\\n\\n        # Convert rewards, advantages, old log_probs to tensors\\n        rewards = torch.tensor(rewards).to(device)\\n        advantages = torch.tensor(advantages).to(device)\\n        old_log_probs = torch.tensor(old_log_probs).to(device)\\n        values = torch.tensor(values).to(device)\\n\\n        # Normalize advantages if necessary\\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\\n\\n        # PPO optimization loop\\n        for _ in range(ppo_epochs):  # PPO typically performs multiple epochs of optimization\\n            for batch in range(len(rewards)):\\n\\n                # Calculate the ratio (new_prob / old_prob)\\n                ratio = torch.exp(log_probs - old_log_probs)\\n\\n                # Calculate surrogate objective with clipping\\n                clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\\n                surrogate_loss = torch.min(ratio * advantages[batch], clipped_ratio * advantages[batch])\\n\\n                # Value loss (typically MSE loss)\\n                value_loss = torch.nn.functional.mse_loss(values[batch], rewards[batch])\\n\\n                # Entropy bonus to encourage exploration (optional)\\n                entropy_loss = -torch.mean(torch.log(ratio))  # Can also be based on entropy from the model\\'s predictions\\n\\n                # Total loss (can include weightings for each term)\\n                loss = -surrogate_loss.meaprinn() + value_loss.mean() - entropy_loss.mean()\\n\\n                # Backpropagation\\n                optimizer.zero_grad()\\n                loss.backward()\\n                optimizer.step()\\n\\n        test_accuracy = evaluate()\\n        print(\\'-\\' * 89)\\n        print(\\'| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}\\'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\\n        print(\\'-\\' * 89)\\n\\n        # Save the model if the test accuracy is the best we\\'ve seen so far.\\n        if not best_test_accuracy or test_accuracy < best_test_accuracy:\\n            with open(\"arithmetic_ppo.pt\", \\'wb\\') as f:\\n                torch.save(model, f)\\n            best_test_accuracy = test_accuracy\\n'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "56bfb36f-50f7-4776-8a94-634a5063bc71",
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "def train_ppo_GRPO(verbose=False):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    best_test_accuracy = None\n",
        "    test_accuracy = evaluate()\n",
        "    print('-' * 89)\n",
        "    print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
        "    print('-' * 89)\n",
        "\n",
        "    # Switch to train mode (enables dropout, etc.)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        start_time = time.time()\n",
        "\n",
        "        for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
        "\n",
        "            # Get a batch of prompts and answers\n",
        "            prompts, _, prompt_length, answers_length, questions, answers = get_batch(\"train\", i, batch_size)\n",
        "            prompts = prompts.to(device)  # (prompt_length, batch_size)\n",
        "\n",
        "            # Generate samples for each prompt\n",
        "            outputs = generate(model,\n",
        "                               prompts,\n",
        "                               new_tokens=answers_length + 1,\n",
        "                               mode=\"sampling\",\n",
        "                               num_samples=num_samples,\n",
        "                               temperature=temperature)\n",
        "\n",
        "            # outputs.shape = (prompt_length + answers_length + 1, batch_size * num_samples)\n",
        "            text_outputs = [tokenizer.decode(outputs[prompt_length:, i].tolist())\n",
        "                            for i in range(outputs.size(1))]\n",
        "\n",
        "            # Compute rewards\n",
        "            rewards = compute_rewards(text_outputs, answers)\n",
        "\n",
        "            # Compute advantages using GRPO method\n",
        "            advantages = calculate_grpo_advantages(rewards)\n",
        "\n",
        "            # Compute log probabilities for the generated outputs\n",
        "            log_probs = compute_log_probs(model, outputs, prompt_length)\n",
        "\n",
        "            # Compute old log probabilities to compute the PPO loss\n",
        "            old_log_probs = log_probs.detach()  # Detach old log probs for comparison\n",
        "\n",
        "            # Compute loss\n",
        "            responses = outputs[prompt_length:, :]\n",
        "            loss = compute_loss_clip_KL(advantages, log_probs, old_log_probs, responses)\n",
        "\n",
        "            # Backpropagate the loss and optimize\n",
        "            optimizer.zero_grad()  # Ensure gradients are zeroed out at the start of the backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print statistics periodically\n",
        "            if i % log_interval == 0 and batch > 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                print('| {:5d}/{:5d} batches | ms/batch {:5.2f}'.format(batch, len(data_train) // batch_size, elapsed))\n",
        "                if verbose:\n",
        "                    print(\"\\nquestion:\", questions[0],\n",
        "                          \"\\nanswer:\", answers[0],\n",
        "                          \"\\noutput:\", text_outputs[:num_samples],\n",
        "                          \"\\nreward:\", rewards[:num_samples],\n",
        "                          \"\\nadvantage:\", advantages[:num_samples], \"\\n\")\n",
        "\n",
        "                start_time = time.time()\n",
        "\n",
        "        # Evaluate the model at the end of each epoch\n",
        "        test_accuracy = evaluate()\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
        "        print('-' * 89)\n",
        "\n",
        "        # Save the model if the test accuracy is the best we've seen so far\n",
        "        if best_test_accuracy is None or test_accuracy > best_test_accuracy:\n",
        "            with open(\"ppo_GRPO.pt\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_test_accuracy = test_accuracy\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:45:46.079513Z",
          "iopub.execute_input": "2025-03-11T10:45:46.079814Z",
          "iopub.status.idle": "2025-03-11T10:45:46.090832Z",
          "shell.execute_reply.started": "2025-03-11T10:45:46.079791Z",
          "shell.execute_reply": "2025-03-11T10:45:46.090201Z"
        },
        "id": "56bfb36f-50f7-4776-8a94-634a5063bc71"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "824ca075",
      "cell_type": "code",
      "source": [
        "'''def train_ppo_GRPO(verbose = False):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    best_test_accuracy = None\n",
        "    test_accuracy = evaluate()\n",
        "    print('-' * 89)\n",
        "    print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
        "    print('-' * 89)\n",
        "\n",
        "    # switch eval for train model (enables dropout)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        start_time = time.time()\n",
        "        for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
        "\n",
        "            # get a batch of prompts and answers\n",
        "            prompts, _, prompt_length, answers_length, questions, answers = get_batch(\"train\", i, batch_size)\n",
        "            prompts = prompts.to(device) # (prompt_length, batch_size)\n",
        "\n",
        "            # generate samples for each prompt\n",
        "            outputs = generate(model,\n",
        "                               prompts,\n",
        "                               new_tokens = answers_length + 1,\n",
        "                               mode = \"sampling\",\n",
        "                               num_samples = num_samples,\n",
        "                               temperature = temperature)\n",
        "            # outputs.shape = (prompt_length + answers_length + 1, batch_size * num_samples)\n",
        "            text_outputs = [tokenizer.decode(outputs[prompt_length:, i].tolist())\n",
        "                            for i in range(outputs.size(1))]\n",
        "\n",
        "            # compute rewards\n",
        "            rewards = compute_rewards(text_outputs, answers)\n",
        "\n",
        "            # compute advantages\n",
        "            advantages = calculate_grpo_advantages(rewards)\n",
        "\n",
        "            # compute log probabilities\n",
        "            log_probs = compute_log_probs(model, outputs, prompt_length)\n",
        "\n",
        "            # compute loss\n",
        "            responses = outputs[prompt_length:, :]\n",
        "            loss = compute_loss_clip_KL(advantages, log_probs, old_log_probs, responses)\n",
        "\n",
        "            # optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if i % log_interval == 0 and batch > 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                print('| {:5d}/{:5d} batches | ms/batch {:5.2f}'.format(batch, len(data_train) // batch_size, elapsed))\n",
        "                if verbose:\n",
        "                    print(\"\\nquestion:\", questions[0],\n",
        "                      \"\\nanswer\", answers[0],\n",
        "                      \"\\noutput:\", text_outputs[:num_samples],\n",
        "                      \"\\nreward:\", rewards[:num_samples],\n",
        "                      \"\\nadvantage:\", advantages[:num_samples], \"\\n\")\n",
        "\n",
        "                start_time = time.time()\n",
        "        test_accuracy = evaluate()\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the test accuracy is the best we've seen so far.\n",
        "        if not best_test_accuracy or test_accuracy < best_test_accuracy:\n",
        "            with open(\"ppo_GRPO.pt\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_test_accuracy = test_accuracy '''\n"
      ],
      "metadata": {
        "id": "824ca075",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:44:01.521389Z",
          "iopub.execute_input": "2025-03-11T10:44:01.521720Z",
          "iopub.status.idle": "2025-03-11T10:44:01.530234Z",
          "shell.execute_reply.started": "2025-03-11T10:44:01.521695Z",
          "shell.execute_reply": "2025-03-11T10:44:01.529536Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b02716ff",
      "cell_type": "code",
      "source": [
        "train_ppo_GRPO(verbose = False)"
      ],
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "b02716ff",
        "outputId": "d3ae59c3-1d5f-4e40-e428-097a04b4edb1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T10:45:59.517786Z",
          "iopub.execute_input": "2025-03-11T10:45:59.518052Z",
          "iopub.status.idle": "2025-03-11T12:30:56.338495Z",
          "shell.execute_reply.started": "2025-03-11T10:45:59.518033Z",
          "shell.execute_reply": "2025-03-11T12:30:56.337611Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "-----------------------------------------------------------------------------------------\n| initialisation | test accuracy  0.19\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch 54.22\n|  1200/ 3600 batches | ms/batch 53.78\n|  1800/ 3600 batches | ms/batch 53.74\n|  2400/ 3600 batches | ms/batch 53.61\n|  3000/ 3600 batches | ms/batch 53.71\n-----------------------------------------------------------------------------------------\n| end of epoch   1 | time: 331.81s | test accuracy  0.00\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch 51.09\n|  1200/ 3600 batches | ms/batch 50.60\n|  1800/ 3600 batches | ms/batch 50.80\n|  2400/ 3600 batches | ms/batch 50.73\n|  3000/ 3600 batches | ms/batch 50.71\n-----------------------------------------------------------------------------------------\n| end of epoch   2 | time: 313.99s | test accuracy  0.10\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch 51.16\n|  1200/ 3600 batches | ms/batch 50.68\n|  1800/ 3600 batches | ms/batch 50.82\n|  2400/ 3600 batches | ms/batch 50.76\n|  3000/ 3600 batches | ms/batch 50.86\n-----------------------------------------------------------------------------------------\n| end of epoch   3 | time: 314.32s | test accuracy  0.10\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch 51.08\n|  1200/ 3600 batches | ms/batch 50.57\n|  1800/ 3600 batches | ms/batch 50.78\n|  2400/ 3600 batches | ms/batch 50.67\n|  3000/ 3600 batches | ms/batch 50.72\n-----------------------------------------------------------------------------------------\n| end of epoch   4 | time: 313.66s | test accuracy  0.26\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch 51.03\n|  1200/ 3600 batches | ms/batch 50.58\n|  1800/ 3600 batches | ms/batch 50.77\n|  2400/ 3600 batches | ms/batch 50.66\n|  3000/ 3600 batches | ms/batch 50.61\n-----------------------------------------------------------------------------------------\n| end of epoch   5 | time: 313.28s | test accuracy  0.34\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch 51.01\n|  1200/ 3600 batches | ms/batch 50.47\n|  1800/ 3600 batches | ms/batch 50.79\n|  2400/ 3600 batches | ms/batch 50.76\n|  3000/ 3600 batches | ms/batch 50.74\n-----------------------------------------------------------------------------------------\n| end of epoch   6 | time: 313.50s | test accuracy  0.70\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch 51.07\n|  1200/ 3600 batches | ms/batch 50.46\n|  1800/ 3600 batches | ms/batch 50.76\n|  2400/ 3600 batches | ms/batch 50.68\n|  3000/ 3600 batches | ms/batch 50.65\n-----------------------------------------------------------------------------------------\n| end of epoch   7 | time: 313.26s | test accuracy  0.87\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch 51.01\n|  1200/ 3600 batches | ms/batch 50.52\n|  1800/ 3600 batches | ms/batch 50.72\n|  2400/ 3600 batches | ms/batch 50.56\n|  3000/ 3600 batches | ms/batch 50.65\n-----------------------------------------------------------------------------------------\n| end of epoch   8 | time: 313.09s | test accuracy  0.96\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch 51.04\n|  1200/ 3600 batches | ms/batch 50.52\n|  1800/ 3600 batches | ms/batch 50.73\n|  2400/ 3600 batches | ms/batch 50.54\n|  3000/ 3600 batches | ms/batch 50.58\n-----------------------------------------------------------------------------------------\n| end of epoch   9 | time: 313.10s | test accuracy  0.99\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch 51.04\n|  1200/ 3600 batches | ms/batch 50.50\n|  1800/ 3600 batches | ms/batch 50.77\n|  2400/ 3600 batches | ms/batch 50.72\n|  3000/ 3600 batches | ms/batch 50.69\n-----------------------------------------------------------------------------------------\n| end of epoch  10 | time: 313.56s | test accuracy  0.98\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch 51.10\n|  1200/ 3600 batches | ms/batch 50.51\n|  1800/ 3600 batches | ms/batch 50.78\n|  2400/ 3600 batches | ms/batch 50.78\n|  3000/ 3600 batches | ms/batch 50.75\n-----------------------------------------------------------------------------------------\n| end of epoch  11 | time: 313.69s | test accuracy  0.95\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch 51.11\n|  1200/ 3600 batches | ms/batch 50.65\n|  1800/ 3600 batches | ms/batch 50.82\n|  2400/ 3600 batches | ms/batch 50.71\n|  3000/ 3600 batches | ms/batch 50.65\n-----------------------------------------------------------------------------------------\n| end of epoch  12 | time: 313.84s | test accuracy  0.99\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch 51.06\n|  1200/ 3600 batches | ms/batch 50.49\n|  1800/ 3600 batches | ms/batch 50.78\n|  2400/ 3600 batches | ms/batch 50.58\n|  3000/ 3600 batches | ms/batch 50.58\n-----------------------------------------------------------------------------------------\n| end of epoch  13 | time: 313.22s | test accuracy  0.97\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch 51.02\n|  1200/ 3600 batches | ms/batch 50.54\n|  1800/ 3600 batches | ms/batch 50.76\n|  2400/ 3600 batches | ms/batch 50.62\n|  3000/ 3600 batches | ms/batch 50.69\n-----------------------------------------------------------------------------------------\n| end of epoch  14 | time: 313.28s | test accuracy  0.99\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch 51.03\n|  1200/ 3600 batches | ms/batch 50.54\n|  1800/ 3600 batches | ms/batch 50.73\n|  2400/ 3600 batches | ms/batch 50.63\n|  3000/ 3600 batches | ms/batch 50.70\n-----------------------------------------------------------------------------------------\n| end of epoch  15 | time: 313.33s | test accuracy  0.99\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch 51.08\n|  1200/ 3600 batches | ms/batch 50.54\n|  1800/ 3600 batches | ms/batch 50.73\n|  2400/ 3600 batches | ms/batch 50.59\n|  3000/ 3600 batches | ms/batch 50.63\n-----------------------------------------------------------------------------------------\n| end of epoch  16 | time: 313.18s | test accuracy  0.96\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch 51.05\n|  1200/ 3600 batches | ms/batch 50.49\n|  1800/ 3600 batches | ms/batch 50.67\n|  2400/ 3600 batches | ms/batch 50.62\n|  3000/ 3600 batches | ms/batch 50.65\n-----------------------------------------------------------------------------------------\n| end of epoch  17 | time: 312.94s | test accuracy  0.98\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch 50.99\n|  1200/ 3600 batches | ms/batch 50.46\n|  1800/ 3600 batches | ms/batch 50.72\n|  2400/ 3600 batches | ms/batch 50.60\n|  3000/ 3600 batches | ms/batch 50.78\n-----------------------------------------------------------------------------------------\n| end of epoch  18 | time: 313.33s | test accuracy  1.00\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch 51.09\n|  1200/ 3600 batches | ms/batch 50.49\n|  1800/ 3600 batches | ms/batch 50.74\n|  2400/ 3600 batches | ms/batch 50.58\n|  3000/ 3600 batches | ms/batch 50.66\n-----------------------------------------------------------------------------------------\n| end of epoch  19 | time: 313.25s | test accuracy  1.00\n-----------------------------------------------------------------------------------------\n|   600/ 3600 batches | ms/batch 51.08\n|  1200/ 3600 batches | ms/batch 50.51\n|  1800/ 3600 batches | ms/batch 50.73\n|  2400/ 3600 batches | ms/batch 50.57\n|  3000/ 3600 batches | ms/batch 50.66\n-----------------------------------------------------------------------------------------\n| end of epoch  20 | time: 313.19s | test accuracy  0.99\n-----------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "aeVn935w5BSp",
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "for i in range(20):\n",
        "    prompt, answers = data_test[i]\n",
        "    prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
        "    output = generate(model, prompt_tensor, len(answers) + 1).view((1,-1))\n",
        "    print(tokenizer.decode(output.tolist()[0]) + \"\\t actual result: \" + answers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeVn935w5BSp",
        "outputId": "2014648b-68e6-4af6-ce9c-b28a3d580c7e",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T12:38:49.767356Z",
          "iopub.execute_input": "2025-03-11T12:38:49.767676Z",
          "iopub.status.idle": "2025-03-11T12:38:50.256760Z",
          "shell.execute_reply.started": "2025-03-11T12:38:49.767652Z",
          "shell.execute_reply": "2025-03-11T12:38:50.255806Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "473+062=535[EOS]\t actual result: 535\n952+626=1578[EOS]\t actual result: 1578\n047+298=345[EOS]\t actual result: 345\n684+038=722[EOS]\t actual result: 722\n998+913=1910[EOS]\t actual result: 1911\n631+917=1548[EOS]\t actual result: 1548\n633+018=651[EOS]\t actual result: 651\n955+024=979[EOS]\t actual result: 979\n923+715=1638[EOS]\t actual result: 1638\n258+551=809[EOS]\t actual result: 809\n003+139=142[EOS]\t actual result: 142\n647+546=1193[EOS]\t actual result: 1193\n375+991=1366[EOS]\t actual result: 1366\n095+163=258[EOS]\t actual result: 258\n197+060=257[EOS]\t actual result: 257\n461+411=872[EOS]\t actual result: 872\n317+864=1181[EOS]\t actual result: 1181\n294+007=301[EOS]\t actual result: 301\n910+393=1303[EOS]\t actual result: 1303\n161+206=367[EOS]\t actual result: 367\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "de0e54de-b3f8-4b22-838a-c23cd5e92da8",
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "de0e54de-b3f8-4b22-838a-c23cd5e92da8"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}